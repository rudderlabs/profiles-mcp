# MCP Interactive Evaluation Framework

Interactive LLM evaluation framework for testing Claude Sonnet-4 interactions with RudderStack Profiles MCP server tools in realistic conversation scenarios.

## Overview

The framework provides comprehensive testing of MCP tool selection and conversation flow with both interactive and batch evaluation modes.

### Key Capabilities

✅ **Interactive Conversations** - Multi-turn conversations with user input when needed  
✅ **Continuous Tool Execution** - Claude automatically continues after tool results  
✅ **Real MCP Tool Execution** - Direct function import, no mocking  
✅ **Flexible Testing Modes** - Both batch and interactive evaluation  
✅ **Complete Conversation Tracking** - Full conversation history with tool calls and results  

## Quick Start

### Prerequisites
```bash
# Environment variables required
export ANTHROPIC_API_KEY="your-key"
export RUDDERSTACK_PAT="your-pat"

# Install dependencies
uv sync
```

### Basic Usage

```bash
# Batch mode (automated testing)
python tests/evaluator.py -q "Tell me about profiles" -o results.json

# Interactive mode (realistic conversations)
python tests/evaluator.py -q "Help me set up profiles project" --interactive

# Multiple iterations for consistency
python tests/evaluator.py -q "How to create LTV model?" -i 5 -o results.json

# CSV test suite
python tests/evaluator.py --csv tests/test_queries.csv -o suite_results.json
```

## Architecture

### Unified Evaluator Design

Single `evaluator.py` file contains:

- **MCPEvaluator**: Main evaluation engine with Claude API integration
- **ConversationTurn**: Individual conversation turns (user/assistant/tool)  
- **EvalResult**: Comprehensive result data with conversation history

### Conversation Flow Engine

```
Initial Prompt → Claude Response → Tool Calls → Tool Execution → 
Tool Results → Claude Processing → Final Response → User Input (optional) → Continue/End
```

### Design Principles

1. **Single File Architecture**: All functionality in `evaluator.py` for simplicity
2. **Real Tool Execution**: Direct function import, no mocking
3. **Conversation Continuity**: Full conversation state maintained across tool calls
4. **Mode Detection**: Automatic detection of interactive vs batch mode
5. **Proper Message Structure**: Claude API compliant tool call formatting

## Usage Modes

### Batch Mode (Default)

Automated testing without user intervention:

```bash
# Single query evaluation
python evaluator.py -q "Tell me about profiles" -o results.json

# Multiple iterations for consistency testing
python evaluator.py -q "How to create LTV model?" -i 5 -o results.json

# CSV test suite with validation
python evaluator.py --csv test_queries.csv -o results.json
```

**Characteristics:**
- Claude executes all available tools automatically
- No user intervention required
- Consistent, reproducible results
- Ideal for regression testing and evaluation suites

### Interactive Mode

Realistic conversation testing with user input:

```bash
# Interactive single query
python evaluator.py -q "Help me set up profiles project" --interactive

# Interactive with conversation history
python evaluator.py -c conversation.md --interactive

# Interactive multiple iterations
python evaluator.py -q "Create LTV model" --interactive -i 3
```

**Features:**
- Claude continues automatically until task completion or input needed
- User prompted only when Claude asks questions or needs clarification
- Type `end` to finish conversation and move to next iteration
- Natural conversation continuation with follow-up questions

## File Structure

### Core Files

| File | Purpose | Usage |
|------|---------|-------|
| `evaluator.py` | **Main evaluation tool** | Primary interface for all testing |
| `run_tests.py` | Test runner orchestrator | Simple test execution |
| `test_constants.py` | Configuration constants | Model versions, settings |
| `prompts.py` | System prompts for testing | Different prompt styles |

### Test Data

| File | Purpose | Format |
|------|---------|--------|
| `test_queries.csv` | Test cases with validation | CSV with expected/forbidden tools |
| `sample_conv.md` | Real conversation examples | Markdown conversation format |
| `test_conv.md` | Additional conversation test | Markdown conversation format |

### Generated Outputs

| File Pattern | Contents | Generated By |
|--------------|----------|-------------|
| `*.json` | Evaluation results | evaluator.py with `-o` flag |
| `*_out.csv` | CSV formatted results | Historical test runs |

## Output Structure

### JSON Results Format

```json
{
  "metadata": {
    "total_results": 1,
    "generated_at": "2025-08-04T15:40:11.196291",
    "model": "claude-4-sonnet-20250514",
    "mode": "batch|interactive",
    "iterations": 1,
    "input_type": "query|conversation|csv"
  },
  "results": [{
    "iteration": 1,
    "agent_reasoning": "Claude's final response after processing tools",
    "tools_called": ["search_profiles_docs", "about_profiles"],
    "tool_params": {"search_profiles_docs": {"query": "LTV model"}},
    "tool_results": {"search_profiles_docs": "Documentation content..."},
    "conversation_turns": 4,
    "user_interventions": 2,
    "ended_by": "completion|user_end|needs_input|error",
    "conversation_history": [
      {"role": "user", "content": "Initial query", "has_tools": false},
      {"role": "assistant", "content": "", "has_tools": true},
      {"role": "tool", "content": "Tool execution results", "has_tools": false},
      {"role": "assistant", "content": "Final response", "has_tools": false}
    ],
    "latency_ms": 9054.67,
    "token_count": 9612,
    "error": null
  }]
}
```

### Key Result Fields

- **`agent_reasoning`**: Claude's final response after processing all tool results
- **`conversation_history`**: Complete conversation flow including tool execution
- **`tools_called`**: All MCP tools used during conversation
- **`user_interventions`**: Number of times user provided input
- **`ended_by`**: How the conversation concluded (completion/user_end/needs_input/error)

## Testing Workflows

### Development Testing

```bash
# 1. Quick single query test
python evaluator.py -q "Your test query" -o quick_test.json

# 2. Interactive testing for complex scenarios
python evaluator.py -q "Complex setup task" --interactive

# 3. Conversation context testing
python evaluator.py -c sample_conv.md -o conv_test.json

# 4. Consistency validation
python evaluator.py -q "Your test query" -i 10 -o consistency.json
```

### Regression Testing

```bash
# Standard test suite
python run_tests.py

# CSV test suite with validation
python evaluator.py --csv test_queries.csv -o regression.json

# Batch consistency testing
python evaluator.py -q "Core functionality test" -i 5 -o regression.json
```

### Performance Analysis

```bash
# Token usage analysis
python evaluator.py -q "Complex query" -i 5 -o performance.json

# Conversation efficiency testing
python evaluator.py -c long_conversation.md --interactive -o efficiency.json

# Tool selection pattern analysis
python evaluator.py --csv tool_selection_tests.csv -o patterns.json
```

## CSV Test Suite Format

### Test File Structure

```csv
test_name,user_prompt,expected_tools,forbidden_tools,description
profiles_basic,Tell me about RudderStack Profiles,about_profiles,,Basic profiles information
connections,What connections are available?,get_existing_connections,about_profiles,Specific tool selection
setup_project,Help me set up a profiles project,profiles_workflow_guide,search_profiles_docs,Workflow guidance
```

### CSV Columns

- **`test_name`**: Unique test identifier
- **`user_prompt`** (or `prompt`): Query to send to Claude
- **`expected_tools`**: Comma-separated tools that should be called (optional)
- **`forbidden_tools`**: Comma-separated tools that should NOT be called (optional)
- **`description`**: Human-readable test description

### Validation Logic

The framework automatically validates tool selection against expected/forbidden criteria:
- ✅ **PASS**: Expected tools called, forbidden tools not called
- ❌ **FAIL**: Expected tools missing or forbidden tools called

## Advanced Features

### Custom System Prompts

Available prompts in `prompts.py`:

| Prompt | Purpose | Use Case |
|--------|---------|----------|
| `CLAUDE_CODE_SYSTEM_PROMPT` | Default - matches Claude Code style | Standard evaluation |
| `VERBOSE_SYSTEM_PROMPT` | Detailed explanations | Comprehensive responses |
| `MINIMAL_SYSTEM_PROMPT` | Concise responses | Minimal interaction testing |
| `TOOL_SELECTION_PROMPT` | Focused on tool selection | Tool choice behavior testing |

### Conversation History Testing

Load real conversations from markdown files:

```markdown
**User**
Help me set up a profiles project

**Assistant**  
I'll help you set up a RudderStack Profiles project...

**User**
Let's focus on customer LTV modeling
```

Usage:
```bash
python evaluator.py -c your_conversation.md --interactive
```

### Multi-Iteration Testing

Test consistency and variation across multiple runs:

```bash
# Consistency testing
python evaluator.py -q "Tell me about profiles" -i 10 -o consistency.json

# Interactive multi-session testing
python evaluator.py -q "Help with setup" --interactive -i 3
```

## What This Framework Enables

### 1. Realistic Conversation Testing
- Multi-turn conversations with natural flow
- User interaction simulation for complex scenarios
- Context-aware tool selection validation

### 2. Tool Chain Validation  
- Test sequences of tool calls and their interactions
- Validate tool result processing and continuation
- Ensure proper conversation state management

### 3. Batch Evaluation
- Automated testing of tool selection and execution
- Regression testing for consistent behavior
- Performance analysis and token usage tracking

### 4. Interactive Scenario Testing
- Test complex scenarios requiring user clarification
- Validate natural conversation flow and continuity
- Real-world usage pattern simulation

### 5. Development & Debugging
- Tool selection pattern analysis
- Prompt engineering validation
- Error handling and edge case testing

## Integration with Main Project

This evaluation framework integrates with the main RudderStack Profiles MCP server:

- **Direct Function Import**: Uses actual MCP tools from `src/main.py`
- **Real Tool Execution**: Executes actual business logic, not mocks
- **Mock Context Only**: Only the MCP request context is mocked
- **Authentication**: Requires `RUDDERSTACK_PAT` for tool authentication
- **Environment Consistency**: Same Python version and dependencies as main project

## Performance Characteristics

- **Average Latency**: 3-10 seconds per evaluation (depending on tool complexity)
- **Token Usage**: 3-15K tokens per interaction
- **Success Rate**: High reliability with proper error handling
- **Consistency**: Reproducible results across iterations
- **Scalability**: Handles both single queries and large test suites

This framework provides comprehensive testing capabilities for MCP tool behavior with both automated and interactive evaluation modes.